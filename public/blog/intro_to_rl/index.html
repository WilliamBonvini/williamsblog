<!DOCTYPE html>
<html lang="en" dir="ltr"><head>
  
                           
     


<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.105.0">
<title>Introduction to Reinforcement Learning | William Bonvini&#39;s Blog</title>


<meta property="twitter:site" content="@William_Bonvini">
<meta property="twitter:creator" content="@William_Bonvini">







  
    
  
<meta name="description" content="A modern, beautiful, and easily configurable blog theme for Hugo.">


<meta property="og:site_name" content="William Bonvini&#39;s Blog">
<meta property="og:title" content="Introduction to Reinforcement Learning | William Bonvini&#39;s Blog">
<meta property="og:description" content="A modern, beautiful, and easily configurable blog theme for Hugo." />
<meta property="og:type" content="page" />
<meta property="og:url" content="https://williambonvini.netlify.app/blog/intro_to_rl/" />
<meta property="og:locale" content="en">




    
        <meta property="og:image" content="https://williambonvini.netlify.app/blog/intro_to_rl/featured.gif" >
        <meta property="twitter:card" content="summary_large_image">
        <meta name="twitter:image" content="https://williambonvini.netlify.app/blog/intro_to_rl/featured.gif" >
    
    
  <meta itemprop="name" content="Introduction to Reinforcement Learning">
<meta itemprop="description" content="High Level Overview An example of reinforcement learning: the agent learns optimally to backflip. Source: Composing Value Functions in Reinforcement Learning. In this blogpost we will introduce the most important concepts in Reinforcement learning and provide some real world examples, to show you how and when it is currently being used.
This series is based on the lectures that DeepMind held for UCL in 2020, they are freely available at this YouTube link."><meta itemprop="datePublished" content="2022-12-03T00:00:00+00:00" />
<meta itemprop="dateModified" content="2022-12-03T00:00:00+00:00" />
<meta itemprop="wordCount" content="998"><meta itemprop="image" content="https://williambonvini.netlify.app/blog/intro_to_rl/featured.gif">
<meta itemprop="keywords" content="" />
  
  
  <!--[if IE]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/img/favicon.ico" type="image/x-icon">
  
  
  <link rel="stylesheet" href="/style.main.min.51048f8224904aac943a02f6057e4a47960d1d64abe43e43820ed1b3017122c1.css" integrity="sha256-UQSPgiSQSqyUOgL2BX5KR5YNHWSr5D5Dgg7RswFxIsE=" media="screen">
  
  
  <script src="/panelset.min.ed1ac24b6e16f4e2481e3d1d098ae66f5bc77438aef619e6e266d8ac5b00dc72.js" type="text/javascript"></script>
  
  
  <script src="/main.min.fe5f9c38ef1d0223ec38e882ad26053310cd2a87955f8d80c58bfb646000db8c.js" type="text/javascript"></script>
</head>
<body>
      <div class="grid-container single">
<header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role="banner">
  <nav class="site-nav db dt-l w-100" role="navigation">
    <a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href="https://williambonvini.netlify.app" title="Home">
      <img src="/img/logo.png" class="dib db-l h2 w-auto" alt="William Bonvini&#39;s Blog">
    </a>
    <div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked">
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/about/" title="About">About</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 active" href="/blog/" title="Blog">Blog</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/research/" title="Research">Research</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/project/" title="Project Portfolio">Projects</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/talk/" title="Talks">Talks</a>
      
      
    </div>
  </nav>
</header>

<main class="page-main pa4" role="main">
  <section class="page-content mw7 center">
    <article class="post-content pa0 ph4-l">
      <header class="post-header">
        <h1 class="f1 lh-solid measure-narrow mb3 fw4">Introduction to Reinforcement Learning</h1>
        
        <p class="f6 measure lh-copy mv1">By William Bonvini in <a href="https://williambonvini.netlify.app/categories/machine-learning">machine learning</a>  <a href="https://williambonvini.netlify.app/categories/reinforcement-learning">reinforcement learning</a> </p>
        <p class="f7 db mv0 ttu">December 3, 2022</p>

      

      </header>
      <section class="post-body pt5 pb4">
        



<h3 id="high-level-overview">High Level Overview
  <a href="#high-level-overview"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<div style="text-align: center;">
  <figure>
    <img src='imgs/back_flip.gif' style="margin-left:auto;margin-right:auto">
    <figcaption>
      <br>
      <i>An example of reinforcement learning: the agent learns optimally to backflip. 
        <br>Source: <a href="https://proceedings.mlr.press/v97/van-niekerk19a.html"> Composing Value Functions in Reinforcement Learning</a>.</i>
    </figcaption>
  </figure>
</div>
<p>In this blogpost we will introduce the most important concepts in Reinforcement learning and provide some real world examples, to show you how and when it is currently being used.</p>
<p>This series is based on the lectures that DeepMind held for UCL in 2020, they are freely available at 
<a href="https://www.youtube.com/watch?v=TCCjZe0y4Qc&amp;t=2641s" target="_blank" rel="noopener">this</a> YouTube link.<br>
I suggest you to watch these videos because they are extremely clear in my opinion. I chose to write this series of blogposts to offer a readable format of the content of DeepMind&rsquo;s lectures.</p>




<h3 id="introduction">Introduction
  <a href="#introduction"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Reinforcement Learning is the branch of Computer Science that studies how to learn to perform tasks by modeling them as the maximization of a reward obtained by interacting with an environment.</p>
<p>This interaction can be summarized in the following diagram:</p>
<div style="text-align:center">
  <figure>
    <img src="imgs/agent_environment_diagram.png" style="zoom:50%">
  </figure>
</div>
<p>Let&rsquo;s describe it, introducing right away some terminology:</p>
<ul>
<li><em><strong>Agent</strong></em><br>
The computer system whose goal is to maximize a reward</li>
<li><em><strong>Environment</strong></em><br>
The system the agent interacts with (by performing <em>actions</em>) and learns from (by observing changes in the environment&rsquo;s state and collecting rewards)</li>
<li><em><strong>Reward</strong></em> \(R_t\)<br>
a scalar feedback signal that indicates how well the agent is doing at timestep \(t\).</li>
</ul>
<p>Shortly, at each timestep \(t\) the agent</p>
<ul>
<li>receives from the environment an observation \(O_t\)</li>
<li>receives a reward \(R_t\)</li>
<li>executes an action \(A_t\)</li>
</ul>
<p>This was a very high level description of the core entities involved in the reinforcement learning problem. But what is it used in? Here are some examples:</p>
<ul>
<li>
<p>
<a href="http://heli.stanford.edu/" target="_blank" rel="noopener">Fly a helicopter</a></p>
</li>
<li>
<p>
<a href="https://arxiv.org/pdf/1909.09571.pdf" target="_blank" rel="noopener">Manage an investment portfolio</a></p>
</li>
<li>
<p>
<a href="https://www.deepmind.com/blog/accelerating-fusion-science-through-learned-plasma-control" target="_blank" rel="noopener">Control a power station</a></p>
</li>
<li>
<p>
<a href="https://www.mathworks.com/help/reinforcement-learning/ug/train-biped-robot-to-walk-using-reinforcement-learning-agents.html" target="_blank" rel="noopener">Make a robot walk</a></p>
</li>
<li>
<p>
<a href="https://www.youtube.com/watch?v=V1eYniJ0Rnk" target="_blank" rel="noopener">Play video or board games</a></p>
</li>
</ul>
<p>Now let&rsquo;s delve into a lower level description, where I will introduce the mathematical terminology that will be used throughout the next episodes.</p>




<h3 id="the-environment">The Environment
  <a href="#the-environment"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>It is a system in which you can perform actions and, consequently, it will</p>
<ul>
<li>change its state</li>
<li>give you a reward</li>
</ul>
<p>This type of system is modeled as a Markov decision process, namely a mathematical framework that models decision making problems.<br>
Look at the image below. It represents a graph with three nodes \((S_0, S_1, S_2)\).<br>
From each state you can perform some actions that will lead you to another state with a certain probability.<br>
We associate a reward (the orange arrows) to some of these actions.</p>
<div style="text-align:center">
  <figure>
    <img src="imgs/mdp.png" style="zoom:25%">
    <figcaption><i>An example of Markov Decision Process.</i></figcaption>
  </figure>
</div>
<p>Rigorously, a Markov Decision Process is defined as a</p>
<ul>
<li>a set of states \(S\) called the <em>set space</em></li>
<li>a set of actions \(A\) called the <em>action space</em></li>
<li>a transition probability distribution<br>
\(P(s&rsquo;|s_t,a_t): S\times A \to S\) that tells you what is the probability to go to state \(s&rsquo;\) if you perform action \(a_t\) in state \(s_t\).</li>
<li>A reward function \(R(s_t, a_t, s&rsquo;):S \times A \times S \to \mathbb{R}\) that returns a scalar feedback (an immediate reward) when you perform action \(a_t\) in state \(s_t\) and end up in state \(s&rsquo;\).</li>
</ul>
<p>Moreover, the transition probability distribution \(P\) must satisfy the Markov property:</p>
<p>$$
P(s&rsquo; |s_t, a_t) = P(s&rsquo;|\mathcal{H}_t, a_t)
$$</p>
<p>$$
\begin{equation}
R(s&rsquo;|s_t, a_t) = R(s&rsquo;|\mathcal{H}_t, a_t)
\end{equation}
$$</p>
<p>This means that</p>
<ol>
<li>the state \(s&rsquo;\) into which we transition after performing action \(a_t\) from state \(s_t\)</li>
<li>the obtained reward \(r\)</li>
</ol>
<p>are independent from the history of the game (namely, the sequence of actions, observations, and rewards obtained so far).</p>
<p>In other words, \(s_t\) contains all the information we need to choose what action to perform next.</p>
<p>We have described extensively how we model an environment in Reinforcement Learning. It&rsquo;s important though to note that the agent could have full knowledge about the environment state or not.</p>
<ul>
<li>
<p><strong>Fully observable environment</strong></p>
<p>The environment is fully observable if
$$
S_t = O_t = \text{environment state}
$$
This means that the agent&rsquo;s state is equivalent to the environment&rsquo;s state.</p>
</li>
<li>
<p><strong>Partially observable environment</strong> <br>
An environment is partially observarble if the observations are not Markovian.<br>
The environment state can still be Markovian, but the agent does not know it.</p>
<p>In this case the agent state is a function of the history.</p>
<p>For instance \(S_t=O_t\), more generally:</p>
</li>
</ul>
<p>$$
S_{t+1}=u(S_t, A_t, R_{t+1}, O_{t+1})
$$</p>
<p>The agent state is often much smaller thank the environment state.</p>




<h3 id="an-example-of-partially-observable-environment">An example of Partially Observable Environment
  <a href="#an-example-of-partially-observable-environment"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>Imagine you are looking at a maze from above.</p>
<div style="text-align:center">
  <figure>
    <img src="imgs/labyrinth_full.png" style="zoom:50%">
    <figcaption><i>The entire labyrinth seen from above: the full environment state.</i></figcaption>
</div>
<figure style="text-align: center">
  <img src="imgs/labyrinth_1.png" width="40%"/>
  <img src="imgs/labyrinth_2.png" width="44%"/>
  <figcaption><i>On the left, two potential state observations. The two observations are indistinguishable, but they refer to two different locations in the labyrinth.</i>   
</figcaption></figure>
These two agent states are not Markovian because the only way we have to understand whether we are on the left side or the right side of the labyrinth is to analyze the agent's history, checking what was the observation at previous timesteps.
<div style="text-align:center">
  <figure>
    <img src="imgs/labyrinth_both.png" width="50%">
  </figure>
</div>
You could not construct a Markov agent state in this maze.  
To deal with partial observability, an agent can construct a suitable state representation \\(\to\\) this is when the concepts of *value function* and *action value function* come in handy.  




<h3 id="the-value-function">The Value Function
  <a href="#the-value-function"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>The value function is a function $$v(s): S \to \mathbb{R}$$ that returns for each state $$s \in S$$ the expected cumulative reward the agent will get by finding himself in such state.</p>
<div>
  \begin{equation}
\begin{split}
v(s) &= \mathbb{E}[G_t | S_t = s] \\
&= \mathbb{E}[R_{t+1} + R_{t+2} + R_{t+3} + \dots |S_t = s]
\end{split}
\end{equation}
</div>




<h3 id="the-policy">The Policy
  <a href="#the-policy"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>A policy is a function \(\pi: S \to A\) that, given a state \(s\), tells the agent what is the optimal action $$a$$ to perform (the optimal action is the one that will lead him to the state with the highest expected cumulative reward).<br>
A policy can be deterministic (the state-action mapping is univoque) or stochastic. In this latter case you will get in output a probability distribution over the actions set \(A\).</p>




<h3 id="the-q-value">The Q value
  <a href="#the-q-value"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h3>
<p>A function that maps a state action pair \((s, a)\) to the expected cumulative reward.</p>
<div>
  \begin{equation}
\begin{split}
q(s,a) &= \mathbb{E}[G_t|S_t=s, A_t=a] \\
&=\mathbb{E}[R_{t+1}+ R_{t+2}+ R_{t+3}+\dots | S_t=s, A_t=a]
\end{split}
\end{equation}
</div>
<p>That&rsquo;s all for this introductory blogpost! See you in the next one.</p>

        
        <details closed class="f6 fw7 input-reset">
  <dl class="f6 lh-copy">
    <dt class="fw7">Posted on:</dt>
    <dd class="fw5 ml0">December 3, 2022</dd>
  </dl>
  <dl class="f6 lh-copy">
    <dt class="fw7">Length:</dt>
    <dd class="fw5 ml0">5 minute read, 998 words</dd>
  </dl>
  
  <dl class="f6 lh-copy">
    <dt class="fw7">Categories:</dt>
    <dd class="fw5 ml0"> <a href="https://williambonvini.netlify.app/categories/machine-learning">machine learning</a>  <a href="https://williambonvini.netlify.app/categories/reinforcement-learning">reinforcement learning</a> </dd>
  </dl>
  
  
  
  <dl class="f6 lh-copy">
    <dt class="fw7">See Also:</dt>
    
  </dl>
</details>

      </section>
      <footer class="post-footer">
        <div class="post-pagination dt w-100 mt4 mb2">
  
  
    <a class="prev dtc pr2 tl v-top fw6"
    href="https://williambonvini.netlify.app/blog/lda/">&larr; Topic Modeling with LDA</a>
  
  
  
    <a class="next dtc pl2 tr v-top fw6"
    href="https://williambonvini.netlify.app/blog/sphinx/">Write better documentation with Sphinx &rarr;</a>
  
</div>

      </footer>
    </article>
    
      
<div class="post-comments pa0 pa4-l mt4">
  
  <script src="https://utteranc.es/client.js"
          repo="apreshill/apero"
          issue-term="pathname"
          theme="boxy-light"
          label="comments :crystal_ball:"
          crossorigin="anonymous"
          async
          type="text/javascript">
  </script>
  
</div>

    
  </section>
</main>
<footer class="site-footer pv4 bt b--transparent ph5" role="contentinfo">
  <nav class="db dt-l w-100">
    <p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">
      &copy; 2023 William Bonvini
      <span class="middot-divider"></span>
      Made with <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/hugo-apero/" rel="dct:source">Hugo Apéro</a></span>.
      <br />
      
Based on <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/formspree/blogophonic-hugo" rel="dct:source">Blogophonic</a></span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://formspree.io" property="cc:attributionName" rel="cc:attributionURL">Formspree</a>.
    </p>
    
    <div class="site-social-links db dtc-l v-mid w-100 w-33-l tc pv2 pv0-l mv0">
      <div class="social-icon-links" aria-hidden="true">
  
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://github.com/WilliamBonvini" title="github" target="_blank" rel="me noopener">
      <i class="fab fa-github fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://open.spotify.com/user/chattershuts" title="spotify" target="_blank" rel="me noopener">
      <i class="fab fa-spotify fa-lg fa-fw"></i>
    </a>
  
</div>

    </div>
    
    <div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0">
      
      <a class="dib pv1 ph2 link" href="/license/" title="License">License</a>
      
      <a class="dib pv1 ph2 link" href="/contact/" title="Contact form">Contact</a>
      
    </div>
  </nav>
  
    <script>

    var i, text, code, codes = document.getElementsByTagName('code');
    for (let i = 0; i < codes.length;) {
      code = codes[i];
      if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
        text = code.textContent;
        if (/^\$[^$]/.test(text) && /[^$]\$$/.test(text)) {
          text = text.replace(/^\$/, '\\(').replace(/\$$/, '\\)');
          code.textContent = text;
        }
        if (/^\\\((.|\s)+\\\)$/.test(text) ||
            /^\\\[(.|\s)+\\\]$/.test(text) ||
            /^\$(.|\s)+\$$/.test(text) ||
            /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
          code.outerHTML = code.innerHTML;  
          continue;
        }
      }
      i++;
    }
</script>

  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



    
  
  
</footer>

      </div>
    </body>
</html>
